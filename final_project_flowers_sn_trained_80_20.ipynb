{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data source\n",
    "\n",
    "Data for download: https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\n",
    "\n",
    "Resources used as reference:\n",
    "1) Training the image classifier to recognize different species of flowers:\n",
    "https://www.kaggle.com/dtosidis/flower-classifier-tensorflow\n",
    "  \n",
    "2) Loading and preprocessing an image dataset\n",
    "https://www.tensorflow.org/tutorials/load_data/images\n",
    "\n",
    "3) Data augmentation\n",
    "https://www.tensorflow.org/tutorials/images/data_augmentation\n",
    "\n",
    "4) Image classification\n",
    "https://www.tensorflow.org/tutorials/images/classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import PIL\n",
    "import PIL.Image\n",
    "\n",
    "import pathlib\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.keras.applications.vgg19 import (\n",
    "    VGG19, \n",
    "    preprocess_input, \n",
    "    decode_predictions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/c/ShankersDocs/EDUCATION/RICE_Bootcamp_DataAnalytics/FinalProject_Img_Recognition_Flowers/Final_RICEproject_ImageRecognition_flowers\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total images in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'flower_photos'\n",
    "data_dir = pathlib.Path(data_dir)\n",
    "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
    "# print(image_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classification\n",
    "https://www.tensorflow.org/tutorials/images/classification?hl=zh-tw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 32\n",
    "img_height = 224\n",
    "img_width = 224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating datasets\n",
    "https://keras.io/examples/vision/image_classification_from_scratch/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3670 files belonging to 5 classes.\n",
      "Using 2936 files for training.\n"
     ]
    }
   ],
   "source": [
    "# When the subset below is defined as \"training\" the 0.2 validation split takes 80% of the data as the training set\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  'flower_photos',\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3670 files belonging to 5 classes.\n",
      "Using 367 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# When the subset below is defined as \"validation\" the 0.1 validation split takes 10% of the data as the validation set\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  'flower_photos',\n",
    "  validation_split=0.1,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3670 files belonging to 5 classes.\n",
      "Using 367 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# When the subset below is defined as \"validation\" the 0.1 validation split takes 10% of the data as the test set\n",
    "\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  'flower_photos',\n",
    "  validation_split=0.1,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']\n"
     ]
    }
   ],
   "source": [
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescaling the data\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Rescaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the data (trainign and validation datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "normalized_train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "normalized_val_ds =  val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_train_ds))\n",
    "first_image = image_batch[0]\n",
    "# Notice the pixels values are now in `[0,1]`.\n",
    "print(np.min(first_image), np.max(first_image)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autotune is done to cache data and make processing and resource mgmt more effieicient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_train_ds = normalized_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "normalized_val_ds = normalized_val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num_classes = 5\n",
    "num_classes = len(class_names)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 (Sequential Model)\n",
    "https://www.tensorflow.org/guide/keras/sequential_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(img_height, img_width, 3)),\n",
    "  tf.keras.layers.Dense(128,activation='relu'),\n",
    "  tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "92/92 [==============================] - 13s 137ms/step - loss: 5208.3359 - accuracy: 0.3219 - val_loss: 1223.1353 - val_accuracy: 0.4223\n",
      "Epoch 2/16\n",
      "92/92 [==============================] - 6s 67ms/step - loss: 1124.1339 - accuracy: 0.3910 - val_loss: 721.5298 - val_accuracy: 0.4360\n",
      "Epoch 3/16\n",
      "92/92 [==============================] - 6s 69ms/step - loss: 989.1677 - accuracy: 0.4142 - val_loss: 454.4877 - val_accuracy: 0.4087\n",
      "Epoch 4/16\n",
      "92/92 [==============================] - 6s 66ms/step - loss: 734.0304 - accuracy: 0.4240 - val_loss: 737.4456 - val_accuracy: 0.3924\n",
      "Epoch 5/16\n",
      "92/92 [==============================] - 6s 67ms/step - loss: 693.9451 - accuracy: 0.4315 - val_loss: 767.1343 - val_accuracy: 0.3569\n",
      "Epoch 6/16\n",
      "92/92 [==============================] - 6s 69ms/step - loss: 516.3157 - accuracy: 0.4244 - val_loss: 578.7139 - val_accuracy: 0.3733\n",
      "Epoch 7/16\n",
      "92/92 [==============================] - 7s 71ms/step - loss: 421.5355 - accuracy: 0.4264 - val_loss: 374.1722 - val_accuracy: 0.2670\n",
      "Epoch 8/16\n",
      "92/92 [==============================] - 6s 69ms/step - loss: 34.2881 - accuracy: 0.2483 - val_loss: 4.0746 - val_accuracy: 0.2098\n",
      "Epoch 9/16\n",
      "92/92 [==============================] - 6s 69ms/step - loss: 1.8413 - accuracy: 0.2210 - val_loss: 3.8713 - val_accuracy: 0.2098\n",
      "Epoch 10/16\n",
      "92/92 [==============================] - 6s 67ms/step - loss: 1.7972 - accuracy: 0.2251 - val_loss: 3.6915 - val_accuracy: 0.2371\n",
      "Epoch 11/16\n",
      "92/92 [==============================] - 6s 67ms/step - loss: 1.7633 - accuracy: 0.2483 - val_loss: 3.5586 - val_accuracy: 0.2343\n",
      "Epoch 12/16\n",
      "92/92 [==============================] - 6s 67ms/step - loss: 1.7312 - accuracy: 0.2480 - val_loss: 3.4428 - val_accuracy: 0.2316\n",
      "Epoch 13/16\n",
      "92/92 [==============================] - 6s 65ms/step - loss: 1.7022 - accuracy: 0.2473 - val_loss: 3.3638 - val_accuracy: 0.2316\n",
      "Epoch 14/16\n",
      "92/92 [==============================] - 6s 64ms/step - loss: 1.6804 - accuracy: 0.2469 - val_loss: 3.2509 - val_accuracy: 0.2316\n",
      "Epoch 15/16\n",
      "92/92 [==============================] - 6s 67ms/step - loss: 1.6618 - accuracy: 0.2469 - val_loss: 3.3024 - val_accuracy: 0.2316\n",
      "Epoch 16/16\n",
      "92/92 [==============================] - 6s 68ms/step - loss: 1.6452 - accuracy: 0.2473 - val_loss: 3.2019 - val_accuracy: 0.2316\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f1c4790a00>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data = val_ds, \n",
    "    epochs=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 (Sequential Model) Epoch sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(img_height, img_width, 3)),\n",
    "  tf.keras.layers.Dense(128,activation='relu'),\n",
    "  tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "92/92 [==============================] - 6s 68ms/step - loss: 3109.6846 - accuracy: 0.3198 - val_loss: 2002.3905 - val_accuracy: 0.3324\n",
      "Epoch 2/6\n",
      "92/92 [==============================] - 6s 64ms/step - loss: 1059.0967 - accuracy: 0.3740 - val_loss: 1328.4467 - val_accuracy: 0.2425\n",
      "Epoch 3/6\n",
      "92/92 [==============================] - 6s 65ms/step - loss: 676.3892 - accuracy: 0.4217 - val_loss: 1104.6982 - val_accuracy: 0.3324\n",
      "Epoch 4/6\n",
      "92/92 [==============================] - 6s 67ms/step - loss: 480.6086 - accuracy: 0.4561 - val_loss: 832.1366 - val_accuracy: 0.3569\n",
      "Epoch 5/6\n",
      "92/92 [==============================] - 6s 66ms/step - loss: 406.3172 - accuracy: 0.4935 - val_loss: 830.6475 - val_accuracy: 0.3706\n",
      "Epoch 6/6\n",
      "92/92 [==============================] - 6s 66ms/step - loss: 464.5512 - accuracy: 0.4816 - val_loss: 1717.7488 - val_accuracy: 0.2834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2cfafbdb880>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data = val_ds, \n",
    "    epochs=6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1a (Sequential Model) With Normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1a = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(img_height, img_width, 3)),\n",
    "  tf.keras.layers.Dense(128,activation='relu'),\n",
    "  tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1a.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "92/92 [==============================] - 6s 67ms/step - loss: 9.3329 - accuracy: 0.2548 - val_loss: 1.5924 - val_accuracy: 0.2834\n",
      "Epoch 2/6\n",
      "92/92 [==============================] - 6s 66ms/step - loss: 1.5261 - accuracy: 0.3392 - val_loss: 1.5553 - val_accuracy: 0.3406\n",
      "Epoch 3/6\n",
      "92/92 [==============================] - 6s 65ms/step - loss: 1.5153 - accuracy: 0.3236 - val_loss: 1.5620 - val_accuracy: 0.3651\n",
      "Epoch 4/6\n",
      "92/92 [==============================] - 6s 65ms/step - loss: 1.4879 - accuracy: 0.3304 - val_loss: 1.5920 - val_accuracy: 0.4033\n",
      "Epoch 5/6\n",
      "92/92 [==============================] - 6s 65ms/step - loss: 1.4419 - accuracy: 0.3675 - val_loss: 1.5167 - val_accuracy: 0.4142\n",
      "Epoch 6/6\n",
      "92/92 [==============================] - 6s 68ms/step - loss: 1.4356 - accuracy: 0.3658 - val_loss: 1.5409 - val_accuracy: 0.4142\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2cfe05e7a00>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1a.fit(\n",
    "    normalized_train_ds,\n",
    "    validation_data = normalized_val_ds, \n",
    "    epochs=6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = tf.keras.Sequential([\n",
    "  layers.experimental.preprocessing.Rescaling(1./255),\n",
    "  layers.Conv2D(32, 3, activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(num_classes)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.compile(\n",
    "  optimizer='adam',\n",
    "  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92/92 [==============================] - 80s 872ms/step - loss: 1.2945 - accuracy: 0.4377 - val_loss: 1.1570 - val_accuracy: 0.5259\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x174f6f01940>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/c/ShankersDocs/EDUCATION/RICE_Bootcamp_DataAnalytics/FinalProject_Img_Recognition_Flowers/Final_RICEproject_ImageRecognition_flowers\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\Mlearning\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     active environment : Mlearning\n",
      "    active env location : C:\\ProgramData\\Anaconda3\\envs\\Mlearning\n",
      "            shell level : 2\n",
      "       user config file : C:\\Users\\nairs\\.condarc\n",
      " populated config files : C:\\Users\\nairs\\.condarc\n",
      "          conda version : 4.8.3\n",
      "    conda-build version : 3.18.11\n",
      "         python version : 3.7.6.final.0\n",
      "       virtual packages : __cuda=10.2\n",
      "       base environment : C:\\ProgramData\\Anaconda3  (writable)\n",
      "           channel URLs : https://repo.anaconda.com/pkgs/main/win-64\n",
      "                          https://repo.anaconda.com/pkgs/main/noarch\n",
      "                          https://repo.anaconda.com/pkgs/r/win-64\n",
      "                          https://repo.anaconda.com/pkgs/r/noarch\n",
      "                          https://repo.anaconda.com/pkgs/msys2/win-64\n",
      "                          https://repo.anaconda.com/pkgs/msys2/noarch\n",
      "          package cache : C:\\ProgramData\\Anaconda3\\pkgs\n",
      "                          C:\\Users\\nairs\\.conda\\pkgs\n",
      "                          C:\\Users\\nairs\\AppData\\Local\\conda\\conda\\pkgs\n",
      "       envs directories : C:\\ProgramData\\Anaconda3\\envs\n",
      "                          C:\\Users\\nairs\\.conda\\envs\n",
      "                          C:\\Users\\nairs\\AppData\\Local\\conda\\conda\\envs\n",
      "               platform : win-64\n",
      "             user-agent : conda/4.8.3 requests/2.22.0 CPython/3.7.6 Windows/10 Windows/10.0.18362\n",
      "          administrator : True\n",
      "             netrc file : None\n",
      "           offline mode : False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:/Users/nairs/Desktop/model_2\\assets\n"
     ]
    }
   ],
   "source": [
    "model_2.save('C:/Users/nairs/Desktop/model_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 - TRAINED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_selection = (\"mobilenet_v2_100_224\", 224) \n",
    "handle_base, pixels = module_selection\n",
    "MODULE_HANDLE =\"https://tfhub.dev/google/imagenet/{}/feature_vector/4\".format(handle_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODULE_HANDLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4 with input size (224, 224)\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = (pixels, pixels)\n",
    "print(\"Using {} with input size {}\".format(MODULE_HANDLE, IMAGE_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_fine_tuning = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGE_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4\n"
     ]
    }
   ],
   "source": [
    "print(\"Building model with\", MODULE_HANDLE)\n",
    "model3 = tf.keras.Sequential([\n",
    "    # Explicitly define the input shape so the model can be properly\n",
    "    # loaded by the TFLiteConverter\n",
    "    tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),\n",
    "    hub.KerasLayer(MODULE_HANDLE, trainable=do_fine_tuning),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(num_classes,\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.build((None,)+IMAGE_SIZE+(3,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer (KerasLayer)     (None, 1280)              2257984   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 6405      \n",
      "=================================================================\n",
      "Total params: 2,264,389\n",
      "Trainable params: 6,405\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(\n",
    "  optimizer=tf.keras.optimizers.SGD(lr=0.005, momentum=0.9), \n",
    "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "92/92 [==============================] - 84s 914ms/step - loss: 0.6750 - accuracy: 0.7568 - val_loss: 0.3393 - val_accuracy: 0.8910\n",
      "Epoch 2/6\n",
      "92/92 [==============================] - 79s 862ms/step - loss: 0.3476 - accuracy: 0.8774 - val_loss: 0.2976 - val_accuracy: 0.9128\n",
      "Epoch 3/6\n",
      "92/92 [==============================] - 108s 1s/step - loss: 0.2964 - accuracy: 0.8965 - val_loss: 0.2718 - val_accuracy: 0.9128\n",
      "Epoch 4/6\n",
      "92/92 [==============================] - 112s 1s/step - loss: 0.2466 - accuracy: 0.9101 - val_loss: 0.2863 - val_accuracy: 0.9128\n",
      "Epoch 5/6\n",
      "92/92 [==============================] - 112s 1s/step - loss: 0.2235 - accuracy: 0.9213 - val_loss: 0.2471 - val_accuracy: 0.9101\n",
      "Epoch 6/6\n",
      "92/92 [==============================] - 111s 1s/step - loss: 0.2116 - accuracy: 0.9247 - val_loss: 0.2647 - val_accuracy: 0.9101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x227a7148f40>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(\n",
    "    normalized_train_ds,\n",
    "    validation_data = normalized_val_ds, \n",
    "    epochs=6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "92/92 [==============================] - 110s 1s/step - loss: 0.2037 - accuracy: 0.9302 - val_loss: 0.2355 - val_accuracy: 0.9210\n",
      "Epoch 2/12\n",
      "92/92 [==============================] - 110s 1s/step - loss: 0.1787 - accuracy: 0.9384 - val_loss: 0.2420 - val_accuracy: 0.9155\n",
      "Epoch 3/12\n",
      "92/92 [==============================] - 110s 1s/step - loss: 0.1708 - accuracy: 0.9401 - val_loss: 0.2450 - val_accuracy: 0.9155\n",
      "Epoch 4/12\n",
      "92/92 [==============================] - 114s 1s/step - loss: 0.1658 - accuracy: 0.9421 - val_loss: 0.2490 - val_accuracy: 0.9264\n",
      "Epoch 5/12\n",
      "92/92 [==============================] - 118s 1s/step - loss: 0.1539 - accuracy: 0.9499 - val_loss: 0.2288 - val_accuracy: 0.9319\n",
      "Epoch 6/12\n",
      "92/92 [==============================] - 116s 1s/step - loss: 0.1460 - accuracy: 0.9520 - val_loss: 0.2297 - val_accuracy: 0.9292\n",
      "Epoch 7/12\n",
      "92/92 [==============================] - 116s 1s/step - loss: 0.1537 - accuracy: 0.9486 - val_loss: 0.2327 - val_accuracy: 0.9346\n",
      "Epoch 8/12\n",
      "92/92 [==============================] - 124s 1s/step - loss: 0.1413 - accuracy: 0.9510 - val_loss: 0.2328 - val_accuracy: 0.9373\n",
      "Epoch 9/12\n",
      "92/92 [==============================] - 125s 1s/step - loss: 0.1333 - accuracy: 0.9578 - val_loss: 0.2847 - val_accuracy: 0.9074\n",
      "Epoch 10/12\n",
      "92/92 [==============================] - 124s 1s/step - loss: 0.1336 - accuracy: 0.9561 - val_loss: 0.2453 - val_accuracy: 0.9319\n",
      "Epoch 11/12\n",
      "92/92 [==============================] - 125s 1s/step - loss: 0.1328 - accuracy: 0.9550 - val_loss: 0.2347 - val_accuracy: 0.9373\n",
      "Epoch 12/12\n",
      "92/92 [==============================] - 125s 1s/step - loss: 0.1280 - accuracy: 0.9588 - val_loss: 0.2509 - val_accuracy: 0.9264\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x227881b99d0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(\n",
    "    normalized_train_ds,\n",
    "    validation_data = normalized_val_ds, \n",
    "    epochs=12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/18\n",
      "92/92 [==============================] - 86s 934ms/step - loss: 0.6653 - accuracy: 0.7565 - val_loss: 0.3333 - val_accuracy: 0.8856\n",
      "Epoch 2/18\n",
      "92/92 [==============================] - 81s 883ms/step - loss: 0.3439 - accuracy: 0.8815 - val_loss: 0.3096 - val_accuracy: 0.8992\n",
      "Epoch 3/18\n",
      "92/92 [==============================] - 81s 885ms/step - loss: 0.2921 - accuracy: 0.8985 - val_loss: 0.2896 - val_accuracy: 0.9019\n",
      "Epoch 4/18\n",
      "92/92 [==============================] - 146s 2s/step - loss: 0.2530 - accuracy: 0.9104 - val_loss: 0.2754 - val_accuracy: 0.9128\n",
      "Epoch 5/18\n",
      "92/92 [==============================] - 214s 2s/step - loss: 0.2397 - accuracy: 0.9166 - val_loss: 0.2743 - val_accuracy: 0.9155\n",
      "Epoch 6/18\n",
      "92/92 [==============================] - 206s 2s/step - loss: 0.2092 - accuracy: 0.9288 - val_loss: 0.2597 - val_accuracy: 0.9292\n",
      "Epoch 7/18\n",
      "92/92 [==============================] - 146s 2s/step - loss: 0.1932 - accuracy: 0.9360 - val_loss: 0.2579 - val_accuracy: 0.9292\n",
      "Epoch 8/18\n",
      "92/92 [==============================] - 82s 889ms/step - loss: 0.1796 - accuracy: 0.9431 - val_loss: 0.2568 - val_accuracy: 0.9264\n",
      "Epoch 9/18\n",
      "92/92 [==============================] - 82s 889ms/step - loss: 0.1674 - accuracy: 0.9438 - val_loss: 0.2457 - val_accuracy: 0.9292\n",
      "Epoch 10/18\n",
      "92/92 [==============================] - 81s 883ms/step - loss: 0.1559 - accuracy: 0.9523 - val_loss: 0.2420 - val_accuracy: 0.9346\n",
      "Epoch 11/18\n",
      "92/92 [==============================] - 82s 887ms/step - loss: 0.1604 - accuracy: 0.9431 - val_loss: 0.2456 - val_accuracy: 0.9264\n",
      "Epoch 12/18\n",
      "92/92 [==============================] - 81s 881ms/step - loss: 0.1456 - accuracy: 0.9499 - val_loss: 0.2444 - val_accuracy: 0.9237\n",
      "Epoch 13/18\n",
      "92/92 [==============================] - 81s 883ms/step - loss: 0.1482 - accuracy: 0.9486 - val_loss: 0.2516 - val_accuracy: 0.9183\n",
      "Epoch 14/18\n",
      "92/92 [==============================] - 168s 2s/step - loss: 0.1370 - accuracy: 0.9544 - val_loss: 0.2420 - val_accuracy: 0.9183\n",
      "Epoch 15/18\n",
      "92/92 [==============================] - 79s 860ms/step - loss: 0.1394 - accuracy: 0.9537 - val_loss: 0.2483 - val_accuracy: 0.9210\n",
      "Epoch 16/18\n",
      "92/92 [==============================] - 80s 865ms/step - loss: 0.1332 - accuracy: 0.9557 - val_loss: 0.2286 - val_accuracy: 0.9346\n",
      "Epoch 17/18\n",
      "92/92 [==============================] - 75s 818ms/step - loss: 0.1218 - accuracy: 0.9598 - val_loss: 0.2466 - val_accuracy: 0.9183\n",
      "Epoch 18/18\n",
      "92/92 [==============================] - 69s 755ms/step - loss: 0.1189 - accuracy: 0.9598 - val_loss: 0.2316 - val_accuracy: 0.9264\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22bb669ec10>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(\n",
    "    normalized_train_ds,\n",
    "    validation_data = normalized_val_ds, \n",
    "    epochs=18\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:/Users/nairs/Desktop/mobilenet_model_80_20\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:/Users/nairs/Desktop/mobilenet_model_80_20\\assets\n"
     ]
    }
   ],
   "source": [
    "model3.save('C:/Users/nairs/Desktop/mobilenet_model_80_20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.save_weights(\"mobilenet_model_80_20.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
