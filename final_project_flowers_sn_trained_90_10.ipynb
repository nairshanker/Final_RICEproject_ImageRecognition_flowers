{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data source\n",
    "\n",
    "Data for download: https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\n",
    "\n",
    "Resources used as reference:\n",
    "1) Training the image classifier to recognize different species of flowers:\n",
    "https://www.kaggle.com/dtosidis/flower-classifier-tensorflow\n",
    "  \n",
    "2) Loading and preprocessing an image dataset\n",
    "https://www.tensorflow.org/tutorials/load_data/images\n",
    "\n",
    "3) Data augmentation\n",
    "https://www.tensorflow.org/tutorials/images/data_augmentation\n",
    "\n",
    "4) Image classification\n",
    "https://www.tensorflow.org/tutorials/images/classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import PIL\n",
    "import PIL.Image\n",
    "\n",
    "import pathlib\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.keras.applications.vgg19 import (\n",
    "    VGG19, \n",
    "    preprocess_input, \n",
    "    decode_predictions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/c/ShankersDocs/EDUCATION/RICE_Bootcamp_DataAnalytics/FinalProject_Img_Recognition_Flowers/Final_RICEproject_ImageRecognition_flowers\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total images in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'flower_photos'\n",
    "data_dir = pathlib.Path(data_dir)\n",
    "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
    "# print(image_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classification\n",
    "https://www.tensorflow.org/tutorials/images/classification?hl=zh-tw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 32\n",
    "img_height = 224\n",
    "img_width = 224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating datasets\n",
    "https://keras.io/examples/vision/image_classification_from_scratch/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3670 files belonging to 5 classes.\n",
      "Using 3303 files for training.\n"
     ]
    }
   ],
   "source": [
    "# When the subset below is defined as \"training\" the 0.2 validation split takes 80% of the data as the training set\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  'flower_photos',\n",
    "  validation_split=0.1,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3670 files belonging to 5 classes.\n",
      "Using 367 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# When the subset below is defined as \"validation\" the 0.1 validation split takes 10% of the data as the validation set\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  'flower_photos',\n",
    "  validation_split=0.1,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3670 files belonging to 5 classes.\n",
      "Using 367 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# When the subset below is defined as \"validation\" the 0.1 validation split takes 10% of the data as the test set\n",
    "\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  'flower_photos',\n",
    "  validation_split=0.1,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']\n"
     ]
    }
   ],
   "source": [
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescaling the data\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Rescaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the data (trainign and validation datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "normalized_train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "normalized_val_ds =  val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_train_ds))\n",
    "first_image = image_batch[0]\n",
    "# Notice the pixels values are now in `[0,1]`.\n",
    "print(np.min(first_image), np.max(first_image)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autotune is done to cache data and make processing and resource mgmt more effieicient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_train_ds = normalized_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "normalized_val_ds = normalized_val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num_classes = 5\n",
    "num_classes = len(class_names)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 (Sequential Model)\n",
    "https://www.tensorflow.org/guide/keras/sequential_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(img_height, img_width, 3)),\n",
    "  tf.keras.layers.Dense(128,activation='relu'),\n",
    "  tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "92/92 [==============================] - 13s 137ms/step - loss: 5208.3359 - accuracy: 0.3219 - val_loss: 1223.1353 - val_accuracy: 0.4223\n",
      "Epoch 2/16\n",
      "92/92 [==============================] - 6s 67ms/step - loss: 1124.1339 - accuracy: 0.3910 - val_loss: 721.5298 - val_accuracy: 0.4360\n",
      "Epoch 3/16\n",
      "92/92 [==============================] - 6s 69ms/step - loss: 989.1677 - accuracy: 0.4142 - val_loss: 454.4877 - val_accuracy: 0.4087\n",
      "Epoch 4/16\n",
      "92/92 [==============================] - 6s 66ms/step - loss: 734.0304 - accuracy: 0.4240 - val_loss: 737.4456 - val_accuracy: 0.3924\n",
      "Epoch 5/16\n",
      "92/92 [==============================] - 6s 67ms/step - loss: 693.9451 - accuracy: 0.4315 - val_loss: 767.1343 - val_accuracy: 0.3569\n",
      "Epoch 6/16\n",
      "92/92 [==============================] - 6s 69ms/step - loss: 516.3157 - accuracy: 0.4244 - val_loss: 578.7139 - val_accuracy: 0.3733\n",
      "Epoch 7/16\n",
      "92/92 [==============================] - 7s 71ms/step - loss: 421.5355 - accuracy: 0.4264 - val_loss: 374.1722 - val_accuracy: 0.2670\n",
      "Epoch 8/16\n",
      "92/92 [==============================] - 6s 69ms/step - loss: 34.2881 - accuracy: 0.2483 - val_loss: 4.0746 - val_accuracy: 0.2098\n",
      "Epoch 9/16\n",
      "92/92 [==============================] - 6s 69ms/step - loss: 1.8413 - accuracy: 0.2210 - val_loss: 3.8713 - val_accuracy: 0.2098\n",
      "Epoch 10/16\n",
      "92/92 [==============================] - 6s 67ms/step - loss: 1.7972 - accuracy: 0.2251 - val_loss: 3.6915 - val_accuracy: 0.2371\n",
      "Epoch 11/16\n",
      "92/92 [==============================] - 6s 67ms/step - loss: 1.7633 - accuracy: 0.2483 - val_loss: 3.5586 - val_accuracy: 0.2343\n",
      "Epoch 12/16\n",
      "92/92 [==============================] - 6s 67ms/step - loss: 1.7312 - accuracy: 0.2480 - val_loss: 3.4428 - val_accuracy: 0.2316\n",
      "Epoch 13/16\n",
      "92/92 [==============================] - 6s 65ms/step - loss: 1.7022 - accuracy: 0.2473 - val_loss: 3.3638 - val_accuracy: 0.2316\n",
      "Epoch 14/16\n",
      "92/92 [==============================] - 6s 64ms/step - loss: 1.6804 - accuracy: 0.2469 - val_loss: 3.2509 - val_accuracy: 0.2316\n",
      "Epoch 15/16\n",
      "92/92 [==============================] - 6s 67ms/step - loss: 1.6618 - accuracy: 0.2469 - val_loss: 3.3024 - val_accuracy: 0.2316\n",
      "Epoch 16/16\n",
      "92/92 [==============================] - 6s 68ms/step - loss: 1.6452 - accuracy: 0.2473 - val_loss: 3.2019 - val_accuracy: 0.2316\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f1c4790a00>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data = val_ds, \n",
    "    epochs=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 (Sequential Model) Epoch sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(img_height, img_width, 3)),\n",
    "  tf.keras.layers.Dense(128,activation='relu'),\n",
    "  tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "92/92 [==============================] - 6s 68ms/step - loss: 3109.6846 - accuracy: 0.3198 - val_loss: 2002.3905 - val_accuracy: 0.3324\n",
      "Epoch 2/6\n",
      "92/92 [==============================] - 6s 64ms/step - loss: 1059.0967 - accuracy: 0.3740 - val_loss: 1328.4467 - val_accuracy: 0.2425\n",
      "Epoch 3/6\n",
      "92/92 [==============================] - 6s 65ms/step - loss: 676.3892 - accuracy: 0.4217 - val_loss: 1104.6982 - val_accuracy: 0.3324\n",
      "Epoch 4/6\n",
      "92/92 [==============================] - 6s 67ms/step - loss: 480.6086 - accuracy: 0.4561 - val_loss: 832.1366 - val_accuracy: 0.3569\n",
      "Epoch 5/6\n",
      "92/92 [==============================] - 6s 66ms/step - loss: 406.3172 - accuracy: 0.4935 - val_loss: 830.6475 - val_accuracy: 0.3706\n",
      "Epoch 6/6\n",
      "92/92 [==============================] - 6s 66ms/step - loss: 464.5512 - accuracy: 0.4816 - val_loss: 1717.7488 - val_accuracy: 0.2834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2cfafbdb880>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data = val_ds, \n",
    "    epochs=6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1a (Sequential Model) With Normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1a = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(img_height, img_width, 3)),\n",
    "  tf.keras.layers.Dense(128,activation='relu'),\n",
    "  tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1a.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "92/92 [==============================] - 6s 67ms/step - loss: 9.3329 - accuracy: 0.2548 - val_loss: 1.5924 - val_accuracy: 0.2834\n",
      "Epoch 2/6\n",
      "92/92 [==============================] - 6s 66ms/step - loss: 1.5261 - accuracy: 0.3392 - val_loss: 1.5553 - val_accuracy: 0.3406\n",
      "Epoch 3/6\n",
      "92/92 [==============================] - 6s 65ms/step - loss: 1.5153 - accuracy: 0.3236 - val_loss: 1.5620 - val_accuracy: 0.3651\n",
      "Epoch 4/6\n",
      "92/92 [==============================] - 6s 65ms/step - loss: 1.4879 - accuracy: 0.3304 - val_loss: 1.5920 - val_accuracy: 0.4033\n",
      "Epoch 5/6\n",
      "92/92 [==============================] - 6s 65ms/step - loss: 1.4419 - accuracy: 0.3675 - val_loss: 1.5167 - val_accuracy: 0.4142\n",
      "Epoch 6/6\n",
      "92/92 [==============================] - 6s 68ms/step - loss: 1.4356 - accuracy: 0.3658 - val_loss: 1.5409 - val_accuracy: 0.4142\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2cfe05e7a00>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1a.fit(\n",
    "    normalized_train_ds,\n",
    "    validation_data = normalized_val_ds, \n",
    "    epochs=6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = tf.keras.Sequential([\n",
    "  layers.experimental.preprocessing.Rescaling(1./255),\n",
    "  layers.Conv2D(32, 3, activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(num_classes)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.compile(\n",
    "  optimizer='adam',\n",
    "  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92/92 [==============================] - 80s 872ms/step - loss: 1.2945 - accuracy: 0.4377 - val_loss: 1.1570 - val_accuracy: 0.5259\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x174f6f01940>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/c/ShankersDocs/EDUCATION/RICE_Bootcamp_DataAnalytics/FinalProject_Img_Recognition_Flowers/Final_RICEproject_ImageRecognition_flowers\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\Mlearning\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     active environment : Mlearning\n",
      "    active env location : C:\\ProgramData\\Anaconda3\\envs\\Mlearning\n",
      "            shell level : 2\n",
      "       user config file : C:\\Users\\nairs\\.condarc\n",
      " populated config files : C:\\Users\\nairs\\.condarc\n",
      "          conda version : 4.8.3\n",
      "    conda-build version : 3.18.11\n",
      "         python version : 3.7.6.final.0\n",
      "       virtual packages : __cuda=10.2\n",
      "       base environment : C:\\ProgramData\\Anaconda3  (writable)\n",
      "           channel URLs : https://repo.anaconda.com/pkgs/main/win-64\n",
      "                          https://repo.anaconda.com/pkgs/main/noarch\n",
      "                          https://repo.anaconda.com/pkgs/r/win-64\n",
      "                          https://repo.anaconda.com/pkgs/r/noarch\n",
      "                          https://repo.anaconda.com/pkgs/msys2/win-64\n",
      "                          https://repo.anaconda.com/pkgs/msys2/noarch\n",
      "          package cache : C:\\ProgramData\\Anaconda3\\pkgs\n",
      "                          C:\\Users\\nairs\\.conda\\pkgs\n",
      "                          C:\\Users\\nairs\\AppData\\Local\\conda\\conda\\pkgs\n",
      "       envs directories : C:\\ProgramData\\Anaconda3\\envs\n",
      "                          C:\\Users\\nairs\\.conda\\envs\n",
      "                          C:\\Users\\nairs\\AppData\\Local\\conda\\conda\\envs\n",
      "               platform : win-64\n",
      "             user-agent : conda/4.8.3 requests/2.22.0 CPython/3.7.6 Windows/10 Windows/10.0.18362\n",
      "          administrator : True\n",
      "             netrc file : None\n",
      "           offline mode : False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:/Users/nairs/Desktop/model_2\\assets\n"
     ]
    }
   ],
   "source": [
    "model_2.save('C:/Users/nairs/Desktop/model_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 - TRAINED MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resource:\n",
    "https://www.tensorflow.org/hub/tutorials/tf2_image_retraining\n",
    "\n",
    "Since training a model from scratch needs a large amount of labeled data, below is a pre-trained TF2 SavedModel from TensorFlow Hub for image feature extraction, trained on the much larger and more general ImageNet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_selection = (\"mobilenet_v2_100_224\", 224) \n",
    "handle_base, pixels = module_selection\n",
    "MODULE_HANDLE =\"https://tfhub.dev/google/imagenet/{}/feature_vector/4\".format(handle_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODULE_HANDLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4 with input size (224, 224)\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = (pixels, pixels)\n",
    "print(\"Using {} with input size {}\".format(MODULE_HANDLE, IMAGE_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_fine_tuning = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGE_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4\n"
     ]
    }
   ],
   "source": [
    "print(\"Building model with\", MODULE_HANDLE)\n",
    "model3 = tf.keras.Sequential([\n",
    "    # Explicitly define the input shape so the model can be properly\n",
    "    # loaded by the TFLiteConverter\n",
    "    tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),\n",
    "    hub.KerasLayer(MODULE_HANDLE, trainable=do_fine_tuning),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(num_classes,\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.build((None,)+IMAGE_SIZE+(3,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer_1 (KerasLayer)   (None, 1280)              2257984   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 6405      \n",
      "=================================================================\n",
      "Total params: 2,264,389\n",
      "Trainable params: 6,405\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(\n",
    "  optimizer=tf.keras.optimizers.SGD(lr=0.005, momentum=0.9), \n",
    "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "104/104 [==============================] - 139s 1s/step - loss: 0.6269 - accuracy: 0.7714 - val_loss: 0.3050 - val_accuracy: 0.8910\n",
      "Epoch 2/6\n",
      "104/104 [==============================] - 125s 1s/step - loss: 0.3416 - accuracy: 0.8834 - val_loss: 0.2900 - val_accuracy: 0.8992\n",
      "Epoch 3/6\n",
      "104/104 [==============================] - 124s 1s/step - loss: 0.2878 - accuracy: 0.8998 - val_loss: 0.2797 - val_accuracy: 0.9074\n",
      "Epoch 4/6\n",
      "104/104 [==============================] - 123s 1s/step - loss: 0.2491 - accuracy: 0.9131 - val_loss: 0.2607 - val_accuracy: 0.9128\n",
      "Epoch 5/6\n",
      "104/104 [==============================] - 122s 1s/step - loss: 0.2222 - accuracy: 0.9258 - val_loss: 0.2522 - val_accuracy: 0.9183\n",
      "Epoch 6/6\n",
      "104/104 [==============================] - 124s 1s/step - loss: 0.2113 - accuracy: 0.9304 - val_loss: 0.2471 - val_accuracy: 0.9183\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x233d61e9400>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(\n",
    "    normalized_train_ds,\n",
    "    validation_data = normalized_val_ds, \n",
    "    epochs=6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "104/104 [==============================] - 127s 1s/step - loss: 0.2045 - accuracy: 0.9279 - val_loss: 0.2350 - val_accuracy: 0.9264\n",
      "Epoch 2/12\n",
      "104/104 [==============================] - 128s 1s/step - loss: 0.1850 - accuracy: 0.9364 - val_loss: 0.2288 - val_accuracy: 0.9292\n",
      "Epoch 3/12\n",
      "104/104 [==============================] - 133s 1s/step - loss: 0.1784 - accuracy: 0.9398 - val_loss: 0.2347 - val_accuracy: 0.9319\n",
      "Epoch 4/12\n",
      "104/104 [==============================] - 132s 1s/step - loss: 0.1705 - accuracy: 0.9431 - val_loss: 0.2356 - val_accuracy: 0.9264\n",
      "Epoch 5/12\n",
      "104/104 [==============================] - 139s 1s/step - loss: 0.1625 - accuracy: 0.9458 - val_loss: 0.2299 - val_accuracy: 0.9292\n",
      "Epoch 6/12\n",
      "104/104 [==============================] - 138s 1s/step - loss: 0.1594 - accuracy: 0.9455 - val_loss: 0.2290 - val_accuracy: 0.9319\n",
      "Epoch 7/12\n",
      "104/104 [==============================] - 139s 1s/step - loss: 0.1460 - accuracy: 0.9497 - val_loss: 0.2168 - val_accuracy: 0.9264\n",
      "Epoch 8/12\n",
      "104/104 [==============================] - 139s 1s/step - loss: 0.1506 - accuracy: 0.9488 - val_loss: 0.2232 - val_accuracy: 0.9319\n",
      "Epoch 9/12\n",
      "104/104 [==============================] - 353s 3s/step - loss: 0.1444 - accuracy: 0.9519 - val_loss: 0.2324 - val_accuracy: 0.9264\n",
      "Epoch 10/12\n",
      "104/104 [==============================] - 422s 4s/step - loss: 0.1413 - accuracy: 0.9561 - val_loss: 0.2187 - val_accuracy: 0.9264\n",
      "Epoch 11/12\n",
      "104/104 [==============================] - 179s 2s/step - loss: 0.1385 - accuracy: 0.9534 - val_loss: 0.2201 - val_accuracy: 0.9346\n",
      "Epoch 12/12\n",
      "104/104 [==============================] - 130s 1s/step - loss: 0.1306 - accuracy: 0.9555 - val_loss: 0.2199 - val_accuracy: 0.9346\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x233d62130a0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(\n",
    "    normalized_train_ds,\n",
    "    validation_data = normalized_val_ds, \n",
    "    epochs=12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/18\n",
      "104/104 [==============================] - 130s 1s/step - loss: 0.1316 - accuracy: 0.9552 - val_loss: 0.2259 - val_accuracy: 0.9237\n",
      "Epoch 2/18\n",
      "104/104 [==============================] - 132s 1s/step - loss: 0.1272 - accuracy: 0.9540 - val_loss: 0.2318 - val_accuracy: 0.9237\n",
      "Epoch 3/18\n",
      "104/104 [==============================] - 133s 1s/step - loss: 0.1188 - accuracy: 0.9616 - val_loss: 0.2176 - val_accuracy: 0.9292\n",
      "Epoch 4/18\n",
      "104/104 [==============================] - 125s 1s/step - loss: 0.1251 - accuracy: 0.9582 - val_loss: 0.2268 - val_accuracy: 0.9373\n",
      "Epoch 5/18\n",
      "104/104 [==============================] - 133s 1s/step - loss: 0.1147 - accuracy: 0.9640 - val_loss: 0.2351 - val_accuracy: 0.9210\n",
      "Epoch 6/18\n",
      "104/104 [==============================] - 130s 1s/step - loss: 0.1089 - accuracy: 0.9643 - val_loss: 0.2350 - val_accuracy: 0.9264\n",
      "Epoch 7/18\n",
      "104/104 [==============================] - 128s 1s/step - loss: 0.1130 - accuracy: 0.9619 - val_loss: 0.2225 - val_accuracy: 0.9264\n",
      "Epoch 8/18\n",
      "104/104 [==============================] - 134s 1s/step - loss: 0.1097 - accuracy: 0.9637 - val_loss: 0.2301 - val_accuracy: 0.9264\n",
      "Epoch 9/18\n",
      "104/104 [==============================] - 135s 1s/step - loss: 0.1064 - accuracy: 0.9649 - val_loss: 0.2227 - val_accuracy: 0.9292\n",
      "Epoch 10/18\n",
      "104/104 [==============================] - 131s 1s/step - loss: 0.1077 - accuracy: 0.9661 - val_loss: 0.2298 - val_accuracy: 0.9292\n",
      "Epoch 11/18\n",
      "104/104 [==============================] - 134s 1s/step - loss: 0.0971 - accuracy: 0.9688 - val_loss: 0.2390 - val_accuracy: 0.9264\n",
      "Epoch 12/18\n",
      "104/104 [==============================] - 136s 1s/step - loss: 0.1110 - accuracy: 0.9667 - val_loss: 0.2252 - val_accuracy: 0.9264\n",
      "Epoch 13/18\n",
      "104/104 [==============================] - 98s 939ms/step - loss: 0.1019 - accuracy: 0.9676 - val_loss: 0.2288 - val_accuracy: 0.9264\n",
      "Epoch 14/18\n",
      "104/104 [==============================] - 90s 864ms/step - loss: 0.1059 - accuracy: 0.9661 - val_loss: 0.2334 - val_accuracy: 0.9346\n",
      "Epoch 15/18\n",
      "104/104 [==============================] - 90s 867ms/step - loss: 0.1025 - accuracy: 0.9667 - val_loss: 0.2281 - val_accuracy: 0.9319\n",
      "Epoch 16/18\n",
      "104/104 [==============================] - 90s 869ms/step - loss: 0.1054 - accuracy: 0.9679 - val_loss: 0.2296 - val_accuracy: 0.9346\n",
      "Epoch 17/18\n",
      "104/104 [==============================] - 90s 868ms/step - loss: 0.1018 - accuracy: 0.9667 - val_loss: 0.2369 - val_accuracy: 0.9264\n",
      "Epoch 18/18\n",
      "104/104 [==============================] - 90s 869ms/step - loss: 0.0946 - accuracy: 0.9721 - val_loss: 0.2371 - val_accuracy: 0.9264\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x233d6207760>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(\n",
    "    normalized_train_ds,\n",
    "    validation_data = normalized_val_ds, \n",
    "    epochs=18\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\Mlearning\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\Mlearning\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\Mlearning\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\Mlearning\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:/Users/nairs/Desktop/mobilenet_model_90_10\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:/Users/nairs/Desktop/mobilenet_model_90_10\\assets\n"
     ]
    }
   ],
   "source": [
    "model3.save('C:/Users/nairs/Desktop/mobilenet_model_90_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
